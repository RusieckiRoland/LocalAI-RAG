# ðŸ§  GPUâ€‘Accelerated RAG with Weaviate (BYOV) + LLaMA

**Local, GPU-accelerated RAG for code repositories (Weaviate BYOV + LLM via llama.cpp).**

This project is a **local-first RAG server** optimized for **analyzing source-code repositories on-premises**, especially in organizations that **cannot send code or derived context to external LLM services** (e.g., regulated industries, critical infrastructure, strict IP policies).

It uses **Weaviate as the single retrieval backend** (vector search + BM25 + hybrid + metadata filtering) with **BYOV (Bring Your Own Vectors)** â€” embeddings are computed locally and stored in Weaviate. **FAISS is not used anywhere** in this project.

### Why it works well for code analysis

The core optimization for code-repo analysis is that retrieval is not just â€œtop-K chunksâ€.  
This system is designed to cooperate with a **.NET + SQL code indexer** that produces:

- **structured code fragments** (files / classes / methods / SQL objects) ready for retrieval,
- rich **metadata** for filtering and access control,
- and â€” most importantly â€” a **dependency graph** linking fragments.

That graph lets retrieval behave more like a real developer: start from a likely entry point and then **trace dependencies** (callers/callees, type usages, EF links, SQL relationships) to assemble a coherent evidence set. In practice this means the model can â€œfollow the codeâ€ instead of guessing based on isolated snippets.

### Not only for code â€” also for regular documents

Although the system is optimized for code repositories, it can also serve as a RAG backend for typical documents. The difference is that it includes optional mechanisms for **relationship-aware retrieval** (graph expansion and dependency tracing) when the indexed data provides those links.

### Local-first server, optional external LLMs, and security policies

The application runs as a **server** that hosts the primary model locally (GPU-accelerated), but it can also be configured to send prompts to **any LLM endpoint**, including external providers.

At the same time, it supports security-oriented policies that help enforce your organizationâ€™s rules:

1. **Block external LLM traffic**  
   The system can be configured to disallow calls to external providers entirely, forcing all inference to remain local.

2. **Classification-aware LLM routing**  
   Retrieved documents can carry **classification labels** (or similar security metadata).  
   If a retrieval batch contains content marked above a configured threshold, the request can be **automatically routed to an approved internal LLM** (as defined by your policies), rather than being sent to an external endpoint.

This makes it possible to combine flexible LLM usage with enforceable constraints, without relying on developer discipline alone.

### Data versioning philosophy: snapshots and snapshot sets

The systemâ€™s data model is built around **snapshots** and **snapshot sets**.

- A **snapshot** is a fixed, versioned corpus you query to get **deterministic, repeatable answers** â€” for example:
  - a specific version of a codebase,
  - or a document set frozen at a specific point in time (e.g., a law as adopted in a given year).

- If you have two versions (e.g., `main` vs `develop`), you are working with **two snapshots**.  
  Likewise, an original law and its amended version are also naturally represented as different snapshots.

- A **snapshot set** groups multiple snapshots into an explicit comparison context.  
  This enables workflows like â€œcompare these two versionsâ€, but the comparison must always be based on **explicit, declared assumptions** (which snapshots, what relationship, what rules) â€” nothing is implicit or accidental.

---

## Architectural Core Principle: Flexibility

The system is built around dynamic pipelines defined in YAML files.  
You can treat pipelines like building blocks (actions) and assemble them as needed:

- easily change the order of processing steps
- add / disable actions (translation, routing, search, answer generationâ€¦)
- define your own prompts, filters, context limits, and policies
- create different work modes (code analysis, UML diagrams, branch comparison)
- reuse and extend existing YAML pipelines via inheritance, eliminating the need to redefine everything from scratch.

Think of it as pipeline composition by configuration, not by code â€” like configuring a workflow in a YAML file.

**At a glance**

```mermaid
flowchart LR
    A[translate_in_if_needed] --> B[load_conversation_history]
    B --> C{sufficiency router}
    C -->|[BM25]/[SEMANTIC]/[HYBRID]| D[search_nodes]
    D --> E[manage_context_budget]
    E --> F[fetch_node_texts]
    F --> G[call_model: answer_v1]
    G --> H{sufficient context?}
    H -->|no| I[loop_guard]
    I --> D
    H -->|yes| J[finalize]
```

### Pipeline reliability additions

- **Context divider for new retrieval batches:** `manage_context_budget` can insert a one-line marker
  (e.g., `<<<New content`) each time a new batch is appended to `state.context_blocks`. This makes
  â€œlatest evidenceâ€ clearly visible to downstream prompts.
- **Sufficiency anti-repeat guard in prompt:** the `sufficiency_router_v1` prompt now includes
  `<<<LAST QUERY>` and `<<<PREVIOUS QUERIES>` injected from pipeline state, and explicitly forbids
  repeating or paraphrasing earlier retrieval queries.

More details, diagrams, and examples are available here:

â†’ **`docs/`** â€“ full documentation of pipelines, actions, and configuration

### Minimal pipeline example (YAML)

```yaml
YAMLpipeline:
  name: "rejewski"

  settings:
    entry_step_id: "translate"
    behavior_version: "0.2.0"
    compat_mode: locked

  steps:
    - id: "translate"
      action: "translate_in_if_needed"
      next: "load_history"

    - id: "load_history"
      action: "load_conversation_history"
      next: "search"

    - id: "search"
      action: "search_nodes"
      search_type: "hybrid"
      top_k: 8
      next: "fetch"

    - id: "fetch"
      action: "fetch_node_texts"
      top_n_from_settings: "node_text_fetch_top_n"
      next: "answer"

    - id: "answer"
      action: "call_model"
      prompt_key: "rejewski/answer_v1"
      next: "finalize"

    - id: "finalize"
      action: "finalize"
      end: true
```

### Pipeline compat/lockfile (required for `compat_mode: locked`)

- `settings.behavior_version` and `settings.compat_mode` are **mandatory**.
- `compat_mode: locked` requires a lockfile next to the YAML:
  - `<pipeline_basename>.lock.json`

Generate the lockfile:

```bash
python -m code_query_engine.pipeline.pipeline_cli lock pipelines/rejewski.yaml
```

**Hardware target.** Optimized for a **single NVIDIA RTX 4090** (CUDA 12.x). Defaults (e.g., full llama.cpp CUDA offload) are tuned for a single high-end GPU (e.g., RTX 4090-class).

**Stack focus.**

* **.NET/C# codebases** â€” deep static analysis via **Roslyn** (ASTs, symbols, references, call graphs) to extract rich, navigable context.
* **SQL & Entity Framework** â€” inspection of schemas, relationships and query usage to surface domain entities and data access patterns to the RAG layer.
* **Dependency graph of code fragments** â€” the index stores **links between chunks** (files, classes, methods) so retrieval can traverse relationships (e.g., callers/callees, type usages, EF entity links) and answer questions with proper context.

**Who is this for?** Organizations that **cannot send code to external services** (e.g., banks, financial institutions, operators of critical infrastructure) and must run **fully local** solutions.

---

## Documentation

- `docs/start/00_run_locally.md` (Start here)
- `docs/start/10_indexing_dotnet_sql.md`
- `docs/start/30_troubleshooting.md`
- `docs/start/40_production.md`
- `docs/start/50_frontend.md`
- `docs/start/60_integrations_plantuml.md`
- `docs/weaviate/weaviate_local_setup.md`

