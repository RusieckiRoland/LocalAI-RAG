name: rag-weaviate
channels:
  - pytorch
  - nvidia
  - conda-forge
dependencies:
  # --- Core environment pins ---
  - python=3.11.*
  - numpy=1.26.4
  - cuda-toolkit=12.1
  - pip

  # --- Pip dependencies ---
  - pip:
      - redis
      - numpy==1.26.4
      - sentence-transformers
      - tqdm
      - flask
      - flask-cors
      - huggingface-hub
      - transformers
      - safetensors
      - sentencepiece
      - sacremoses
      - protobuf
      - mdpo
      - pytest
      - pytest-mock
      - pytest-cov
      - diskcache         # ✅ Added manually for llama-cpp-python
      - jinja2            # ✅ Dependency also needed by llama-cpp-python
      - typing-extensions # ✅ Safe for PyTorch and llama-cpp-python
      - python-dotenv
      - weaviate-client
      - pyjwt[crypto]

# === Notes ===
# 1) After creating the environment, install llama-cpp-python manually:
#       conda activate rag-weaviate
#       pip install --no-deps https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu121/llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl
#
# 2) Verify GPU visibility:
#       nvidia-smi
#       python -c "import torch; print(torch.version.cuda, torch.cuda.is_available())"
#
# 3) Keep numpy pinned (<2.0) until all native modules you use officially support NumPy 2.x
#    (e.g., llama_cpp_python, torch).
